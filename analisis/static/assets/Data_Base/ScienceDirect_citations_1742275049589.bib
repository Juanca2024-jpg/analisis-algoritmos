@incollection{DOMINOWSKI19941,
title = {CHAPTER 1 - History of Research on Thinking and Problem Solving},
editor = {Robert J. Sternberg},
booktitle = {Thinking and Problem Solving},
publisher = {Academic Press},
address = {San Diego},
pages = {1-35},
year = {1994},
volume = {2},
series = {Handbook of Perception and Cognition},
isbn = {978-0-08-057299-4},
doi = {https://doi.org/10.1016/B978-0-08-057299-4.50007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080572994500074},
author = {Roger L. Dominowski and Lyle E. Bourne},
abstract = {Publisher Summary
The two problems that characterize the modern psychology of thinking are mental representation and mental computation. Throughout the history of psychology, there has been an agreement that the essential features of a problem are that an organism has a goal but lacks a clear or well-learned route to the goal. Thus the emphasis in research on problem solving has been on response discovery—how the organism arrives at an effective, goal-attaining behavior. There have often been controversies over the role of learning or past experiences in problem solving. This chapter illustrates the conflict between emphases on learning and emphases on perception as central components of problem solving. Because a problem solver must find a solution, it might seem inevitable that an essential activity tries different approaches makes errors until the right approach is found. Earlier in problem-solving research, trial and error became associated with the view that acquiring a solution was a gradual, undirected process that did not involve perception or comprehension of problem requirements or structure.}
}
@incollection{ANDERSON1996237,
title = {Chapter 8 - Interference and Inhibition in Memory Retrieval},
editor = {Elizabeth Ligon Bjork and Robert A. Bjork},
booktitle = {Memory},
publisher = {Academic Press},
address = {San Diego},
pages = {237-313},
year = {1996},
isbn = {978-0-12-102570-0},
doi = {https://doi.org/10.1016/B978-012102570-0/50010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780121025700500100},
author = {Michael C. Anderson and James H. Neely},
abstract = {Publisher Summary
This chapter discusses the causes of memory interference and the extent of situations in which these mechanisms operate. First, the chapter discusses some widely held assumptions about the situation of interference, focusing on the idea that such effects arise from competition for access via a shared retrieval cue. This notion is sufficiently general that it may be applied in a variety of interference settings, which is illustrated briefly. Then the classical interference paradigms from which these ideas emerged are reviewed. The chapter also reviews more recent phenomena that both support and challenge classical conceptions of interference. These phenomena provide compelling illustrations of the generality of interference and, consequently, of the importance of understanding its mechanisms. A recent perspective on interference is highlighted that builds upon insights from modern work, while validating intuitions underlying several of the classical interference mechanisms. According to this new perspective, forgetting derives not from acquiring new memories per se, but from the impact of later retrievals of the newly learned material. After discussing findings from several paradigms that support this retrieval-based view, the chapter illustrates how forgetting might be linked to inhibitory processes underlying selective attention.}
}
@article{CHEN2020100002,
title = {Application and theory gaps during the rise of Artificial Intelligence in Education},
journal = {Computers and Education: Artificial Intelligence},
volume = {1},
pages = {100002},
year = {2020},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2020.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X20300023},
author = {Xieling Chen and Haoran Xie and Di Zou and Gwo-Jen Hwang},
keywords = {Artificial intelligence in education, Systematic review, Application gap, Theory gap},
abstract = {Considering the increasing importance of Artificial Intelligence in Education (AIEd) and the absence of a comprehensive review on it, this research aims to conduct a comprehensive and systematic review of influential AIEd studies. We analyzed 45 articles in terms of annual distribution, leading journals, institutions, countries/regions, the most frequently used terms, as well as theories and technologies adopted. We also evaluated definitions of AIEd from broad and narrow perspectives and clarified the relationship among AIEd, Educational Data Mining, Computer-Based Education, and Learning Analytics. Results indicated that: 1) there was a continuingly increasing interest in and impact of AIEd research; 2) little work had been conducted to bring deep learning technologies into educational contexts; 3) traditional AI technologies, such as natural language processing were commonly adopted in educational contexts, while more advanced techniques were rarely adopted, 4) there was a lack of studies that both employ AI technologies and engage deeply with educational theories. Findings suggested scholars to 1) seek the potential of applying AI in physical classroom settings; 2) spare efforts to recognize detailed entailment relationships between learners’ answers and the desired conceptual understanding within intelligent tutoring systems; 3) pay more attention to the adoption of advanced deep learning algorithms such as generative adversarial network and deep neural network; 4) seek the potential of NLP in promoting precision or personalized education; 5) combine biomedical detection and imaging technologies such as electroencephalogram, and target at issues regarding learners’ during the learning process; and 6) closely incorporate the application of AI technologies with educational theories.}
}
@article{201943,
title = {Literature Listing},
journal = {World Patent Information},
volume = {56},
pages = {43-59},
year = {2019},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S017221901830156X}
}
@article{FINKEL2017303,
title = {L’analyse cognitive, la psychologie numérique et la formation des enseignants à l’université},
journal = {Pratiques Psychologiques},
volume = {23},
number = {3},
pages = {303-323},
year = {2017},
note = {Préparer la nouvelle génération de psychologues : objectifs, méthodes et ressources dans l'enseignement de la psychologie},
issn = {1269-1763},
doi = {https://doi.org/10.1016/j.prps.2017.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S126917631730055X},
author = {A. Finkel},
keywords = {Pensée informatique, Psychologie numérique, Analyse cognitive, Enseignement, Université, Digital thinking, Computer science, Digital psychology, Cognitive analysis, Teaching, College, University},
abstract = {Résumé
L’analyse cognitive est le fruit d’une réflexion utilisant, entre autres, les principes de la pensée informatique (mathématique, logique et algorithmique) pour repenser des connaissances et des techniques de plusieurs domaines de la psychologie cognitive, de la psychologie sociale, de la psychologie des émotions et de la psychologie de la communication en vue de la formation pédagogique des enseignants d’université. Avant de présenter l’analyse cognitive, je vais rappeler comment l’histoire du calcul depuis 5000 ans peut être vue comme une tentative (réussie) de comprendre, formaliser et automatiser le traitement humain de l’information, thème sur lequel se retrouvent l’informatique et la psychologie. Puis, je montrerai qu’un nouveau comportementalisme, basé sur le comportement numérique mais aussi sur les comportements classiques et sur les zones du cerveau, est en train d’émerger et qu’il produit des modèles psychologiques qui sont utilisés pour connaître les pensées et les intentions des personnes observées, pour mieux leur vendre des objets et pour prédire leurs actions par exemple ; une psychologie numérique émerge. Bien que chacun d’entre nous utilise son ordinateur portable et internet comme une extension de son esprit (lire « Petite Poucette » de Michel Serres), nous n’avons pas encore intégré explicitement la pensée informatique. Nous expliciterons donc quelques éléments de la pensée informatique et nous présenterons enfin l’analyse cognitive en soulignant comment elle utilise à la fois la pensée informatique et des connaissances de psychologie.
Cognitive analysis is the result of an afterthought using, amongst other things, principles from computer science (maths, logic and algorithmic) to rethink techniques and knowledge of several areas of psychology: cognitive psychology, social psychology, psychology of emotions and communication. The aim is pedagogy training for university teachers. Before explaining what cognitive analysis is, I will tell you how the history of calculation since 5000 years may be seen as a (successful) stab at understanding, formalizing and automating human processing of data, which is where computer science and psychology meet. Then, I will demonstrate that a new behavioralism, based on digital and classical behavior and specific areas of the brain, is starting to emerge and produces psychological models that are used to know the thoughts and intents of the people observed, to better sell them stuff and predict their actions. A digital psychology emerges. Although all of us use a laptop and Internet as an extension of our minds (read “Petite Poucette” by Michel Serres), the digital thought remains implicit. We will explicit several components of digital thinking and will show how cognitive analysis uses digital thinking and knowledge from psychology.}
}
@article{PROFETA2018111,
title = {Bernstein’s levels of movement construction: A contemporary perspective},
journal = {Human Movement Science},
volume = {57},
pages = {111-133},
year = {2018},
issn = {0167-9457},
doi = {https://doi.org/10.1016/j.humov.2017.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167945717305717},
author = {Vitor L.S. Profeta and Michael T. Turvey},
keywords = {Bernstein, Control, Coordination, Synergy, Movement construction},
abstract = {Explanation of how goal-directed movements are made manifest is the ultimate aim of the field classically referred to as “motor control”. Essential to the sought-after explanation is comprehension of the supporting functional architecture. Seven decades ago, the Russian physiologist and movement scientist Nikolai A. Bernstein proposed a hierarchical model to explain the construction of movements. In his model, the levels of the hierarchy share a common language (i.e., they are commensurate) and perform complementing functions to bring about dexterous movements. The science of the control and coordination of movement in the phylum Craniata has made considerable progress in the intervening seven decades. The contemporary body of knowledge about each of Bernstein’s hypothesized functional levels is both more detailed and more sophisticated. A natural consequence of this progress, however, is the relatively independent theoretical development of a given level from the other levels. In this essay, we revisit each level of Bernstein’s hierarchy from the joint perspectives of (a) the ecological approach to perception-action and (b) dynamical systems theory. We review a substantial and relevant body of literature produced in different areas of study that are accommodated by this ecological-dynamical version of Bernstein’s levels. Implications for the control and coordination of movement and the challenges to producing a unified theory are discussed.}
}
@article{GRAY2019124,
title = {BrainQuest: The use of motivational design theories to create a cognitive training game supporting hot executive function},
journal = {International Journal of Human-Computer Studies},
volume = {127},
pages = {124-149},
year = {2019},
note = {Strengthening gamification studies: critical challenges and new opportunities},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918304555},
author = {Stuart Iain Gray and Judy Robertson and Andrew Manches and Gnanathusharan Rajendran},
keywords = {Gamification, Motivational theory, Game design, Cognitive training games, Executive functions},
abstract = {For children to yield greater mental performance abilities in real world settings, training approaches should offer practice in problems which have an affective component requiring social interactions, and be motivating over a sustained period. Current cognitive training games often overlook the important relationship between cognition and emotion, characterised by ‘hot executive function’, and correlated with fundamental academic and life outcomes. Here, we present robust qualitative evidence from a case study which documents the social relationships, motivation and engagement of a class of ten-year-old children who used an active smartphone cognitive training game called BrainQuest in their physical education lessons over a period of 5 weeks. Game design elements which are intended to move beyond simple gamification of cognitive tests are presented, along with a discussion of how these design elements worked in practice. The paper also presents and discusses the impact of the game upon the cognitive and emotional regulatory skills, characterised by executive function skills, based on the findings of this initial work. We conclude with recommendations for the designers of cognitive training games in the future and discussion of appropriate research methods for future gamification studies.}
}
@article{ROBSON2022101018,
title = {Searching for the principles of a less artificial A.I.},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101018},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101018},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001617},
author = {B. Robson and G. Ochoa-Vargas},
keywords = {AI, Algorithms, X factor, Emergent properties, Consciousness, Quantum effects},
abstract = {What would it take to build a computer physician that can take its place amongst human peers? Currently, Neural Nets, especially as so-called “Deep Learning” nets, dominate what is popularly called “Artificial Intelligence”, but to many critics they seem to be little more than powerful data-analytic tools inspired by some of the more basic functions and regions of the human brain such as those involved in early processes in biological vision, classification, and categorization. The deeper nature of human intelligence as the term is normally meant, including relating to consciousness, has been the domain of philosophers, psychologists, and some neuroscientists. Now, attention is turning to neuronal mechanisms in humans and simpler organisms as a basis of a truer AI with far greater potential. Arguably, the approach required should be rooted in information theory and algorithmic science. But as discussed in this paper, caution is required: “just any old information” might not do. The information might need to be of a particular dynamical and actioning nature, and that might significantly impact the kind of computation and computer hardware required. Overall, however, the authors do not favor emergent properties such as those based on complexity and quantum effects. Despite the possible difficulties, such studies could, in return, have substantial benefits for biology and medicine beyond the computational tools that they produce to serve those disciplines.}
}
@article{ESTEFO2019226,
title = {The Robot Operating System: Package reuse and community dynamics},
journal = {Journal of Systems and Software},
volume = {151},
pages = {226-242},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300342},
author = {Pablo Estefo and Jocelyn Simmonds and Romain Robbes and Johan Fabry},
keywords = {Robot Operating System, Package management, Software ecosystems},
abstract = {ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem.}
}
@article{ROMEROORGANVIDEZ2024112029,
title = {Data visualization guidance using a software product line approach},
journal = {Journal of Systems and Software},
volume = {213},
pages = {112029},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112029},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000724},
author = {David Romero-Organvidez and Jose-Miguel Horcas and José A. Galindo and David Benavides},
keywords = {Effective communication, Graphs, Tables, Software product line, Variability, Visualization},
abstract = {Data visualization aims to convey quantitative and qualitative information effectively by determining which techniques and visualizations are most appropriate for different situations and why. Various software solutions can produce numerous visualizations of the same data set. However, data visualization encompasses a wide range of visual configurations that depend on factors such as the type of data being displayed, the different displays (e.g., scatter plots, line graphs, and pie charts), the visual components used to represent the data (e.g., lines, dots, and bars), and the specific visual attributes of those components (e.g., color, shape, size, and length). A similar problem arises when designing data tables, where the dimensionality of the data and its complexity influence the choice of the most appropriate structure (e.g., unidirectional, bidirectional). Often, this broad spectrum of configurations requires a visualization expert who knows which techniques are best for which type of data source and what is to be conveyed. Typically, researchers and developers lack knowledge of data visualization best practices and must learn the design principles that enable effective communication and the technical details of the specific software tool they use to generate visualizations. This paper proposes a software product line approach to model and realize the variability of the visualization design process, using feature models to encode knowledge about design best practices in graphs and charts. Our approach involves solving visualization design variability through a stepwise configuration process and evaluating the proposal for a specific software visualization tool. Our solution facilitates effective communication of quantitative results by helping researchers and developers select and generate the most effective visualizations for each case. This approach opens up new opportunities for research at the intersection of data visualization and variability.}
}
@article{DESPEAUX2007359,
title = {Abstracts},
journal = {Historia Mathematica},
volume = {34},
number = {3},
pages = {359-374},
year = {2007},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0315086007000353},
author = {Sloan Evans Despeaux and Laura Martini and Kim Plofker}
}
@article{BARRICELLI2019101,
title = {End-user development, end-user programming and end-user software engineering: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {101-137},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302577},
author = {Barbara Rita Barricelli and Fabio Cassano and Daniela Fogli and Antonio Piccinno},
keywords = {Systematic mapping study, End-user development, End-user programming, End-user software engineering},
abstract = {End-User Development (EUD), End-Programming (EUP) and End-User Software Engineering (EUSE) are three related research fields that study methods and techniques for empowering end users to modify and create digital artifacts. This paper presents a systematic mapping study aimed at identifying and classifying scientific literature about EUD, EUP and EUSE in the time range January 2000–May 2017. We selected 165 papers found through a manual selection of papers from specific conferences, journal special issues, and books, integrated with an automatic search on the most important digital libraries. The answer to our research question was built through a classification of the selected papers on seven dimensions: type of approach, interaction technique, phase in which the approach is adopted, application domain, target use, class of users, and type of evaluation. Our findings suggest that EUD, EUP and EUSE are active research topics not only in Human–Computer Interaction, but also in other research communities. However, little cross-fertilization exists among the three themes, as well as unifying frameworks and approaches for guiding novice designers and practitioners. Other findings highlight trends and gaps related to the analysis’ dimensions, which have implications on the design of future tools and suggest open issues for further investigations.}
}
@article{ALDAAJEH2022102754,
title = {The role of national cybersecurity strategies on the improvement of cybersecurity education},
journal = {Computers & Security},
volume = {119},
pages = {102754},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102754},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822001493},
author = {Saleh AlDaajeh and Heba Saleous and Saed Alrabaee and Ezedin Barka and Frank Breitinger and Kim-Kwang {Raymond Choo}},
keywords = {Cybersecurity strategic plan, Cybersecurity education, NICE framework, Cybersecurity curricula, GQO+Strategies paradigm},
abstract = {Digital information and telecommunication technologies have not only become essential to individuals’ daily lives but also to a nation’s sustained economic growth, societal well-being, critical infrastructure resilience, and national security. Consequently, the protection of a nation’s cyber sovereignty from malicious acts is a major concern. This signifies the importance of cybersecurity education in facilitating the creation of a resilient cybersecurity ecosystem and in supporting cyber sovereignty. This study reviews a sample from world-leading countries National Cybersecurity Strategic Plans (NCSPs) and analyzes the associated existing cybersecurity education and training improvement initiatives. Furthermore, a proposal to adopt the Goal-Question-Outcomes(GQO)+Strategies paradigm into cybersecurity education and training programs curricula improvement to national cybersecurity strategic goals is presented. The proposal maps cybersecurity strategic goals to cybersecurity skills and competencies using the National Initiative for Cybersecurity Education (NICE) framework. The newly proposed cybersecurity education and training programs’ curricula learning outcomes were generated from the GQO+Strategies paradigm based on the three major cybersecurity strategic goals: Development of secure digital and information technology infrastructure and services, defending from sophisticated cyber threats, and enrichment of individuals’ cybersecurity maturity and awareness. It is highly recommended that cybersecurity university program administrators utilize the proposed GQO+Strategies to align their program’s curriculum to NCSP. Hence, closing the gap that exists with the relevant skills and sustain national cybersecurity workforces.}
}
@article{MOHDAMINUDDIN2023103582,
title = {The rise of website fingerprinting on Tor: Analysis on techniques and assumptions},
journal = {Journal of Network and Computer Applications},
volume = {212},
pages = {103582},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103582},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523000012},
author = {Mohamad Amar Irsyad {Mohd Aminuddin} and Zarul Fitri Zaaba and Azman Samsudin and Faiz Zaki and Nor Badrul Anuar},
keywords = {Security, Privacy, Anonymity, Tor, Traffic analysis, Website fingerprinting},
abstract = {Tor is one of the most popular anonymity networks that allows Internet users to hide their browsing activity. Hiding browsing activity is essential for Internet users to increase their privacy. Only Tor users should know the website they are browsing. However, an attacker can utilise the Website Fingerprinting (WF) attack to identify webpages browsed by Tor users. WF is a significant threat to Internet users' privacy as Tor should conceal the browsed webpages' information. Existing WF studies focused on the investigation to improve the identification capabilities, overlooking the systematic discussion and assessment of existing techniques. In addition, existing surveys and analyses reviewed insufficient variation of WF on Tor techniques. Therefore, this survey paper aims to provide a systematic and thorough review of various WF on Tor techniques. First, we discuss WF on Tor techniques in five primary aspects: threat model, victim target, website realm, traffic feature, and traffic classifier. We analyse and classify the reviewed studies on each WF aspect. The classification facilitates in-depth understanding and comparison between WF on Tor techniques. Furthermore, this paper investigates nine assumptions exercised in WF on Tor: closed-world, sequential browsing, isolated traffic, replicability, traffic parsing, passive webpage, disabled cache, static content, and single webpage. These assumptions limit the WF on Tor's practicality in real-world scenarios. Our analysis and classification indicate that most WF on Tor studies apply these assumptions despite being only suitable in controlled environments or laboratory experiments. In addition, most reviewed studies often lack transparency on the assumptions exercised in their studies, risking misunderstanding the WF on Tor techniques' actual practicality. At the end of this survey, we present WF on Tor taxonomy and highlight 21 WF on Tor research's limitations and gaps with plausible recommendations. We also discuss the WF on Tor studies' contribution category and development phase.}
}
@incollection{CUEVAS2025147,
title = {7 - Basic models},
editor = {Erik Cuevas and Karla Avila and Miguel Islas Toski and Héctor Escobar},
booktitle = {Agent-Based Models with MATLAB},
publisher = {Morgan Kaufmann},
pages = {147-232},
year = {2025},
isbn = {978-0-443-24004-1},
doi = {https://doi.org/10.1016/B978-0-443-24004-1.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443240041000072},
author = {Erik Cuevas and Karla Avila and Miguel Islas Toski and Héctor Escobar},
keywords = {Agent-based models, Classic models, Neighborhood, Principles, Randomness},
abstract = {In this chapter, we will delve into the art of constructing agent-based models (Helbing, 2012). Agent-based modeling is a powerful and versatile tool that allows us to simulate complex systems by representing individual agents and their interactions within a given environment. Here, we will take a step-by-step approach, starting with classic models and gradually moving toward creating customized versions. By understanding how these models work and learning how to modify and extend them, we will gain valuable insights into alternative scenarios and even uncover entirely new phenomena.}
}
@article{STEPHENS2021103466,
title = {Landscape changes and their hydrologic effects: Interactions and feedbacks across scales},
journal = {Earth-Science Reviews},
volume = {212},
pages = {103466},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2020.103466},
url = {https://www.sciencedirect.com/science/article/pii/S0012825220305122},
author = {C.M. Stephens and U. Lall and F.M. Johnson and L.A. Marshall},
abstract = {Human activities have extensively altered landscapes throughout the world and further changes are expected in the future. Anthropogenic impacts such as land use change, groundwater extraction and dam construction, along with the effects of climate change, interact with natural factors including soil weathering and erosion. Together, these processes create a constantly shifting, dynamic terrestrial environment that violates the assumption of stationarity commonly applied in hydrology. Consequently, hydrologists need to rethink both statistical and calibrated models to account for complex environmental processes. We review the literature on human-landscape-hydrological interactions to identify processes and feedbacks that influence water balances. Most of the papers covered consider only a few of these processes at a time and focus on structural attributes of the interactions rather than the short and long-term dynamics. We identify challenges in representing the scale-dependence, environmental connectivity and human-water interactions that characterize complex, dynamic landscapes. A synthesis of the findings posits connections between different landscape changes, as well as the associated timescales and level of certainty. A case study explores how different processes could combine to drive long-term shifts in catchment behavior. Recognizing that some important questions remain unaddressed by traditional approaches, we suggest the concept of ‘big laboratories’ in which multifaceted experiments are conducted in the environment by artificially inducing landscape change. These experiments would be accompanied by mechanistic modeling to both untangle experimental results and improve the theoretical basis of environmental models. An ambitious program of physical and virtual experimentation is needed to progress hydrologic prediction for dynamic landscapes.
Plain language summary
The Earth’s surface is constantly changing due to human-driven and natural processes. Shifts may be driven by humans directly (e.g. via land use change) or indirectly (e.g. by driving climate change that causes shifts in ecological communities). Other changes are natural, such as certain soil processes that lead to shifts in texture and properties over time. In many places, landscape change is now occurring at unprecedented rates. This impacts the water cycle, creating a need for models that are robust under changing conditions. Our paper synthesizes a wide range of literature on key aspects of landscape change that have wide-ranging implications for hydrology. We focus on the impacts of processes at different spatial and temporal scales, along with feedbacks between various environmental and anthropogenic shifts. We discuss connections between different landscape changes and the timescales over which they each affect the water cycle. A case study is presented to highlight the potential for cascading landscape disturbances that could alter long-term catchment response. Recognizing limitations in traditional data collection and modeling, we introduce the concept of ‘big laboratories’ to conduct environmental experiments under landscape change, providing an avenue for addressing the complex questions around hydrology in a changing world.}
}
@incollection{ZEYER2015235,
title = {11 - For the mutual benefit: Health information provision in the science classroom},
editor = {Catherine {Arnott Smith} and Alla Keselman},
booktitle = {Meeting Health Information Needs Outside Of Healthcare},
publisher = {Chandos Publishing},
pages = {235-261},
year = {2015},
isbn = {978-0-08-100248-3},
doi = {https://doi.org/10.1016/B978-0-08-100248-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002483000111},
author = {Albert Zeyer and Daniel M. Levin and Alla Keselman},
keywords = {Health education, Health literacy, Information, Knowledge, Science education},
abstract = {In this chapter, the authors argue that the school science classroom should help students deal with complex real-life information about health and disease. They also discuss means by which curriculum and instruction in science education can be tied to these issues. The chapter reviews opportunities and challenges presented to individuals by the expectations of participatory health care, focusing on models of health literacy that can help understand and address the challenges. The authors argue that the problem of ensuring effective information use often lies in a transmission approach to health information provision. Transmitted knowledge is often not understood nor applied, as demonstrated in studies of human papillomavirus vaccination education. An alternative to knowledge transmission is the approach that aims to foster critical literacy, which is grounded in critical thinking essential to the practice of science. The chapter reviews a number of interdisciplinary science education activities that introduce health issues in the context of biology, physics, and chemistry education, ensuring deep understanding needed for developing critical literacy. It also discusses science education approaches and theories that encourage the development of deep, culturally meaningful science knowledge. Finally, the chapter reviews professional development and the role of various professionals, including teachers and librarians, in the collaborative endeavor of effective health information provision.}
}
@incollection{2020729,
title = {Index},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {729-744},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-815614-8.18001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156148180016}
}
@article{WAITE2020103838,
title = {Difficulties with design: The challenges of teaching design in K-5 programming},
journal = {Computers & Education},
volume = {150},
pages = {103838},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103838},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520300385},
author = {Jane Waite and Paul Curzon and William Marsh and Sue Sentance},
keywords = {K-5 computing education, Teachers, Design, Programming},
abstract = {Teachers in England are required to ensure that learners from the age of five are taught about algorithms and program design. Yet, there is evidence that despite teachers reporting that design is important, they are not converting this into classroom practice. This paper describes a survey study, in which we explored teachers’ difficulties in using design. We surveyed 207 teachers asking them free-text questions on their use of design in teaching programming and their views of pupils’ responses to using design. In the survey, we also investigated teachers’ understanding of the term algorithm, an essential concept which may be a contributing factor in their difficulties with design. We provide underpinning data on the difficulties of using design that teachers of pupils aged from 5 to 11 years old (Grades K to 5) have in teaching programming. Difficulties with design identified include pupil resistance, a lack of time, a lack of pupil and teacher expertise, conflicting pedagogical choices and a general confusion over what an algorithm is. There were statistically significant differences in selection of the term ‘algorithm’ to describe programming artefacts whether a teacher was a specialist or a generalist, what training they had received on programming or design, the age group taught and programming language used. Teachers were more likely to call a complex code snippet an ‘algorithm’ than a simpler one and more likely to select the term to describe code snippets than a design artefact. We make suggestions of how to alleviate the problems including that teachers are introduced to the idea of ambiguous representations of algorithms and a process which refines the representation from ambiguous to unambiguous as the design progresses.}
}
@incollection{CARETTE202215,
title = {Chapter Two - Embracing the laws of physics: Three reversible models of computation},
editor = {Ali R. Hurson},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {126},
pages = {15-63},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000838},
author = {Jacques Carette and Roshan P. James and Amr Sabry},
keywords = {Reversible programming, Reversible Boolean circuits, Monoidal categories, Type isomorphisms, Commutative semirings, Homotopy-type theory, Quantum circuits, Permutations},
abstract = {Our main models of computation (the Turing Machine and the RAM) and most modern computer architectures make fundamental assumptions about which primitive operations are realizable on a physical computing device. The consensus is that these primitive operations include logical operations like conjunction, disjunction and negation, as well as reading and writing to a large collection of memory locations. This perspective conforms to a macro-level view of physics and indeed these operations are realizable using macro-level devices involving thousands of electrons. This point of view is however incompatible with computation realized using quantum devices or analyzed using elementary thermodynamics as both these fundamental physical theories imply that information is a conserved quantity of physical processes and hence of primitive computational operations. Our aim is to redevelop foundational computational models in a way that embraces the principle of conservation of information. We first define what information is and what its conservation means in a computational setting. We emphasize the idea that computations must be reversible transformations on data. One can think of data as modeled using topological spaces and programs as modeled by reversible deformations of these spaces. We then illustrate this idea using three notions of data and their associated reversible computational models. The first instance only assumes unstructured finite data, i.e., discrete topological spaces. The corresponding notion of reversible computation is that of permutations. We show how this simple model subsumes conventional computations on finite sets. We then consider a modern structured notion of data based on the Curry–Howard correspondence between logic and type theory. We develop the corresponding notion of reversible deformations using a sound and complete programming language for witnessing type isomorphisms and proof terms for commutative semirings. We then “move up a level” to examine spaces that treat programs as data, which is a crucial notion for any universal model of computation. To derive the corresponding notion of reversible programs between programs, i.e., reversible program equivalences, we look at the “higher dimensional” analog to commutative semirings: symmetric rig groupoids. The coherence laws for these groupoids turn out to be exactly the sound and complete reversible program equivalences we seek. We conclude with some possible generalizations inspired by homotopy type theory and survey several open directions for further research.}
}
@incollection{WU2022293,
title = {Chapter 8 - Chronotopologic krigology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {293-344},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000034},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Geostatistical Kriging, Space-time ordinary, Simple, Indicator chronoblock and functional Kriging, Accuracy indicators, Cross-validation},
abstract = {Various types of chronotopologic Kriging are presented, including ordinary, simple and indicator Kriging of natural attribute distributions, as well as chronoblock and functional Kriging. Their specific properties, links to chronotopologic mapping and real-world application ranges are reviewed. Interpolation accuracy indicators and cross-validation tests are analyzed. The benefits and concerns of applied Krigology are outlined.}
}
@incollection{CAIRNS19863,
title = {CHAPTER 1 - A Contemporary Perspective on Social Development},
editor = {Phillip S. Strain and Michael J. Guralnick and Hill M. Walker},
booktitle = {Children's Social Behavior},
publisher = {Academic Press},
pages = {3-47},
year = {1986},
isbn = {978-0-12-673455-3},
doi = {https://doi.org/10.1016/B978-0-12-673455-3.50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780126734553500051},
author = {Robert B. Cairns}
}
@incollection{MOORE2024493,
title = {Chapter 17 - The application of knowledge in soil microbiology, ecology, and biochemistry (SMEB) to the solution of today’s and future societal needs},
editor = {Eldor A. Paul and Serita D. Frey},
booktitle = {Soil Microbiology, Ecology and Biochemistry (Fifth Edition)},
publisher = {Elsevier},
edition = {Fifth Edition},
pages = {493-536},
year = {2024},
isbn = {978-0-12-822941-5},
doi = {https://doi.org/10.1016/B978-0-12-822941-5.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822941500017X},
author = {John C. Moore and Nathaniel Mueller},
keywords = {Biogeochemistry, climate change, science literacy, microbial communities, soil organic matter, food webs, sustainability development goals},
abstract = {This chapter presents an ecosystem-based framework for applying soil microbiology, ecology, and biochemistry (SMEB) principles and processes to address the preservation and sustainability of global soils to meet societal needs in the face of environmental change. The approach is organized around the UN Sustainable Development Goals, focusing on ecosystem services provided by soil microbes and soil biota at the nexus of food, water, and energy. The role of soil biota in ecosystem processes and the impacts of human activities on those processes are presented. The approach promotes conservation and regenerative methods that manage natural SMEB processes to optimize ecosystem services and minimize alterations in natural processes. Adoption and application of sound SMEB science will require a high degree of literacy among different stakeholders to codevelop management practices and regulatory policy. The approach also advocates promoting SMEB science literacy within the education system through reforms in science standards and curricula.}
}
@article{KHRENNIKOV2006225,
title = {Quantum-like brain: “Interference of minds”},
journal = {Biosystems},
volume = {84},
number = {3},
pages = {225-241},
year = {2006},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2005.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0303264705001942},
author = {Andrei Khrennikov},
abstract = {We present a contextualist statistical realistic model for quantum-like representations in physics, cognitive science, and psychology. We apply this model to describe cognitive experiments to check quantum-like structures of mental processes. The crucial role is played by interference of probabilities for mental observables. Recently one such experiment based on recognition of images was performed. This experiment confirmed our prediction on the quantum-like behavior of mind. In our approach “quantumness of mind” has no direct relation to the fact that the brain (as any physical body) is composed of quantum particles. We invented a new terminology “quantum-like (QL) mind.” Cognitive QL-behavior is characterized by a nonzero coefficient of interference λ. This coefficient can be found on the basis of statistical data. There are predicted not only cos⁡θ-interference of probabilities, but also hyperbolic cosh⁡θ-interference. This interference was never observed for physical systems, but we could not exclude this possibility for cognitive systems. We propose a model of brain functioning as a QL-computer (there is a discussion on the difference between quantum and QL computers).}
}
@article{CANNAVO2020102450,
title = {A visual editing tool supporting the production of 3D interactive graphics assets for public exhibitions},
journal = {International Journal of Human-Computer Studies},
volume = {141},
pages = {102450},
year = {2020},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2020.102450},
url = {https://www.sciencedirect.com/science/article/pii/S1071581920300525},
author = {Alberto Cannavò and Francesco De Pace and Federico Salaroglio and Fabrizio Lamberti},
keywords = {End-user development (EUD), Visual programming languages (VPLs), Interactive assets, 3D Graphics, Virtual reality (VR), Augmented reality (AR), Human-machine interaction (HMI), Natural user interfaces (NUIs)},
abstract = {The introduction of interactive assets in public exhibitions is capable to significantly enhance the visitors’ user experience. However, the creation of interactive applications could represent a challenging task, especially for users lacking computer skills. Visual programming languages (VPLs) – one of the instruments belonging to the broad categories of methods and tools devised to support end-user development (EUD) – promise to offer an intuitive way to overcome these limitations, by providing easy-to-use and efficient interfaces for encoding applications’ logic. Moving from these considerations, this paper first analyses pros and cons of tools devised so far to support the generation of interactive contents. Then, it presents the design of a new tool named Visual Scene Editor (VSE), which allows users with little to no programming skills to create 3D interactive applications by combining available assets through an interactive, visual process. Both objective and subjective measurements have been collected with both skilled and unskilled users to evaluate the performance of the proposed tool. A comparison with existing solutions shows a reduction in the time required to complete the assigned tasks, of the complexity of the logic created, as well as of the number of errors made, confirming the suitability of the VSE for the said purpose.}
}
@article{MCCART2013235,
title = {Goal attainment on long tail web sites: An information foraging approach},
journal = {Decision Support Systems},
volume = {55},
number = {1},
pages = {235-246},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2013.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167923613000535},
author = {J.A. McCart and B. Padmanabhan and D.J. Berndt},
keywords = {Information Foraging Theory, Long tail, Data mining, Clickstream analysis},
abstract = {The long tail has attracted substantial theoretical as well as practical interest, yet there have been few empirical studies that have explicitly examined the factors that drive online conversions at these sites. This research tests several hypotheses derived from Information Foraging Theory (IFT) that pertain to goal achievement on long tail Web sites. IFT introduced concepts of information patches and information scent to model information seeking behavior of individuals, but has mostly been tested in production rule environments where the theory is used to simulate user behavior. Testing IFT-driven hypotheses on real data required learning information patches and scents using an inductive approach and in this paper we adapt existing algorithms for these discovery tasks. Our results based on clickstream data from forty-seven small business Web sites show both the existence of valuable information patches and information scent trails as well as their importance in explaining conversion on these sites. The majority of the hypotheses were supported and we discuss the implications of this for researchers and practitioners.}
}
@article{REISENZEIN20096,
title = {Emotions as metarepresentational states of mind: Naturalizing the belief–desire theory of emotion},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {6-20},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000272},
author = {Rainer Reisenzein},
keywords = {Emotion, Belief–desire theory, Metacognition, Affective computing, BDI},
abstract = {Describes the outlines of a computational explication of the belief–desire theory of emotion, a variant of cognitive emotion theory. According to the proposed explication, a core subset of emotions including surprise are nonconceptual products of hardwired mechanisms whose primary function is to subserve the monitoring and updating of the central representational system of humans, the belief–desire system. The posited emotion-producing mechanisms are analogous to sensory transducers; however, instead of sensing the world, they sense the state of the belief–desire system and signal important changes in this system, in particular the fulfillment and frustration of desires and the confirmation and disconfirmation of beliefs. Because emotions represent this information about the state of the representational system in a nonconceptual format, emotions are nonconceptual metarepresentations. It is argued that this theory of emotions provides for a deepened understanding of the role of emotions in cognitive systems and solves several problems of psychological emotion theory.}
}
@incollection{COOPER201313,
title = {On Computable Numbers, with an Application to the Entscheidungsproblem – A Correction},
editor = {S. Barry Cooper and Jan Van Leeuwen},
booktitle = {Alan Turing: His Work and Impact},
publisher = {Elsevier},
address = {Boston},
pages = {13-115},
year = {2013},
isbn = {978-0-12-386980-7},
doi = {https://doi.org/10.1016/B978-0-12-386980-7.50002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123869807500022},
author = {S. Barry Cooper and Jan Van Leeuwen}
}
@incollection{WILLIAMS202351,
title = {Part Two—The international benchmarking exercise},
editor = {David Baker and Lucy Ellis and Caroline Williams and Cliff Wragg},
booktitle = {Benchmarking Library, Information and Education Services},
publisher = {Chandos Publishing},
pages = {51-107},
year = {2023},
isbn = {978-0-323-95662-8},
doi = {https://doi.org/10.1016/B978-0-323-95662-8.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956628000151},
author = {Caroline Williams and Cliff Wragg}
}
@article{ROSSON2008468,
title = {Design planning by end-user web developers},
journal = {Journal of Visual Languages & Computing},
volume = {19},
number = {4},
pages = {468-484},
year = {2008},
note = {Selected Papers from IEEE Symposium on Visual Languages and Human Centric Computing 2007 (VL/HCC 2007)},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X08000141},
author = {Mary Beth Rosson and Hansa Sinha and Mithu Bhattacharya and Dejin Zhao},
keywords = {End-user programming, Web development, Design, Concept maps},
abstract = {We report an exploratory research project that investigates the impacts of different forms of design planning on end users asked to develop a simple interactive web application. End users created their projects (a Ride Board application) using the CLICK end-user web development tool [J. Rode, User-centered design of end-user web development tool, Ph.D. Dissertation, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, 2005]. Some participants were asked to create a conceptual map to plan their projects and others to write user interaction scenarios; a third group was asked to do whatever they found useful. We describe the planning that each group underwent, how they approached the web development task, and their reactions to the experience afterwards. The overall pattern of results suggests that while the participants who planned using scenarios felt they better understood the web development task, it was the group who created concept maps that explored and incorporated more of the novel programming features of the CLICK tool. We also discuss the role of gender in the CLICK development task, noting that women were less likely to explore the tool's novel features and perceived themselves as less successful in the task. We conclude with a discussion of design implications and future work.}
}
@incollection{KIRK201771,
title = {Chapter 4 - Memory and data locality},
editor = {David B. Kirk and Wen-mei W. Hwu},
booktitle = {Programming Massively Parallel Processors (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {71-101},
year = {2017},
isbn = {978-0-12-811986-0},
doi = {https://doi.org/10.1016/B978-0-12-811986-0.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119860000042},
author = {David B. Kirk and Wen-mei W. Hwu},
keywords = {Memory bandwidth, memory-bound, on-chip memory, tiling, strip-mining, shared memory, private memory, scope, lifetime, occupancy},
abstract = {This chapter introduces the concepts of memory bound application. It uses matrix multiplication to illustrate opportunities for reducing the number of global memory accesses. It then introduces the tiling technique where barrier synchronization is used to coordinate the timing of executing threads for improved locality and reduced global memory accesses. The tiling techniques, however, involve additional complexities in boundary checks. The chapter uses matrix multiplication to illustrate the additional boundary checks needed for a tiled kernel to be applicable to arbitrary matrix sizes. The chapter concludes with an overview of how usage of shared memory and registers can affect the number of thread blocks that can be accommodated in each Streaming Multiprocessor.}
}
@incollection{2024552,
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {552-598},
year = {2024},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.09001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895090015}
}
@article{WANG2017182,
title = {Bee and Frog Co-Evolution Algorithm and its application},
journal = {Applied Soft Computing},
volume = {56},
pages = {182-198},
year = {2017},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2017.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S1568494617301096},
author = {Hong-bo Wang and Xue-Na Ren and Xu-yan Tu},
keywords = {Artificial Bee Colony, Shuffled Frog Leaping, Evolutionary computing, Resources scheduling},
abstract = {In order to obtain better generalization abilities and mitigate the impacts of the best and worst individuals during the process of optimization, this paper suggests Bee and Frog Co-Evolution Algorithm (abbreviation for BFCEA), which combines Mnemonic Shuffled Frog Leaping algorithm With Cooperation and Mutation (abbreviation for MSFLACM) with improved Artificial Bee Colony (abbreviation for ABC). The contrast experimental study about different iteratively updating strategies was acted in BFCEA, including strategy of integrating with ABC, regeneration of the worst frog and its leaping step. The key techniques focus on the first 10 and the last 10 frogs evolving ABC in BFCEA, namely, the synchronous renewal strategy for those winner and loser should be applied, after certain G times’ MSFLACM-running, so as to avoid trapping local optimum in later stage. The ABC evolution process will be called between all memes’ completing inner iteration and all frogs’ outer shuffling, the crossover operation is removed from MSFLACM for its little effect on time-consuming and convergence in this novel algorithm. Besides, in ABC, the scout bee is generated by Cauchy mutating instead at random. The performance of proposed approach is examined by well-known 16 numerical benchmark functions, and obtained results are compared with basic Shuffled Frog Leaping algorithm (abbreviation for SFLA), ABC and four other variants. The experimental results and related application in cloud resource scheduling show that the proposed algorithm is effective and outperforms other variants, in terms of solution quality and convergence, and the improved variants can obtain a lower degree of unbalanced load and relatively stable scheduling strategy of resources in complicated cloud computing environment.}
}
@article{SMITH1991251,
title = {The owl and the electric encyclopedia},
journal = {Artificial Intelligence},
volume = {47},
number = {1},
pages = {251-288},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90056-P},
url = {https://www.sciencedirect.com/science/article/pii/000437029190056P},
author = {Brian Cantwell Smith},
abstract = {A review of “On the thresholds of knowledge”, by D.B. Lenat and E.A. Feigenbaum.}
}
@article{LEECULTURA2022100355,
title = {Children’s play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach},
journal = {International Journal of Child-Computer Interaction},
volume = {31},
pages = {100355},
year = {2022},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100355},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000647},
author = {Serena Lee-Cultura and Kshitij Sharma and Michail Giannakos},
keywords = {Multimodal data, MMLA, Learning Analytics, Sensors, Cognitive Load Theory, Educational Technologies, Embodied Interaction, Multi-Modal mixed methods approach},
abstract = {Motion-Based Learning Technologies (MBLT) offer a promising approach for integrating play and problem-solving behaviour within children’s learning. The proliferation of sensor technology has driven the field of learning technology towards the development of tools and methods that may benefit from the produced Multi-Modal Data (MMD). Such data can be used to uncover cognitive, affective and physiological processes during learning activities. Combining MMD with more traditionally exercised assessment tools, such as video content analysis, provides a more holistic understanding of children’s learning experiences and has the potential to enable the design of educational technologies capable of harmonising children’s cognitive, affective and physiological processes, while promoting appropriately balanced play and problem-solving efforts. However, the use of an MMD mixed methods approach that combines qualitative and MMD data to understand children’s behaviours during engagement with MBLT is rather unexplored. We present an in-situ study where 26 children, ages 10–12, solved a motion-based sorting task for learning geometry. We continuously and unobtrusively monitored children’s learning experiences using MMD collection via eye-trackers, wristbands, Kinect joint tracking, and a web camera. We devised SP3, a novel observational scheme that can be used to understand children’s solo interactions with MBLT, and applied it to identify and extract children’s evoked play and problem-solving behaviour. Collective analysis of the MMD and video codes provided explanations of children’s task performance through consideration of their holistic learning experience. Lastly, we applied predictive modelling to identify the synergies between various MMD measurements and children’s play and problem-solving behaviours. This research sheds light on the opportunities offered in the confluence of video coding (a traditional method in learning sciences) and MMD (an emerging method that leverages sensors proliferation) for investigating children’s behaviour with MBLT.}
}
@incollection{WANG2021157,
title = {Chapter 5 - Battery state-of-charge estimation methods},
editor = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
booktitle = {Battery System Modeling},
publisher = {Elsevier},
pages = {157-198},
year = {2021},
isbn = {978-0-323-90472-8},
doi = {https://doi.org/10.1016/B978-0-323-90472-8.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323904728000093},
author = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
keywords = {State-of-charge estimation, Coordinate transformation, Binary iterative algorithm, Extended Kalman filtering, Equivalent modeling, Correction strategy, Thermal influencing effect, Time-varying current condition, Complex current rate verification},
abstract = {With the large-scale promotion of new energy vehicles, the demand for power batteries such as the lithium-ion type has increased. Its service lifespan is closely related to the service condition. The important embodiment of the service condition is the battery state of charge, which can reflect its residual capacity. Through accurate residual capacity, the battery application strategy can be planned to realize the battery operation of the best condition. The long service lifespan can be realized by adjusting the voltage and current. Therefore, real-time accurate state estimation has a significant effect on battery management. The extended Kalman filtering algorithm is introduced to estimate the state value under complex working conditions. The overview of the state-of-charge estimation is first conducted. After that, the equivalent modeling construction method is explored. The coordinate transformation treatment is implemented in the binary iterative calculation algorithm, according to which the algorithm is implemented. In the extended calculation process, the iterative prediction and correction strategies are introduced into the test. Consequently, the pulse-power characteristic test is conducted to obtain the estimation features, considering a thermal influencing effect, a time-varying current condition, and a complex current rate verification.}
}
@incollection{DIETRICH19943,
title = {CHAPTER 1 - Thinking Computers and The Problem of Intentionality},
editor = {Eric Dietrich},
booktitle = {Thinking Computers and Virtual Persons},
publisher = {Academic Press},
pages = {3-34},
year = {1994},
isbn = {978-0-12-215495-9},
doi = {https://doi.org/10.1016/B978-0-12-215495-9.50006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780122154959500065},
author = {Eric Dietrich},
abstract = {Publisher Summary
This chapter is an attempt to clear the names of artificial intelligence (AI) and computational cognitive science. These two related disciplines have been accused of a conceptual error so profound that their very existence is jeopardized. Sometimes, however, philosophers successfully arrest and lock up the guilty. The best example of this, ironically, is in psychology. Artificial intelligence and computational cognitive science are both committed to the claim that computers can think. The former is committed to the claim that human-made computers can think, while computational cognitive science is committed to the view that naturally occurring computers, brains, think. AI is the field dedicated to building intelligent computers. AI ultimately wants a machine that could solve very difficult, novel problems like proving Fermat's last theorem, correcting the greenhouse effect, or figuring out the fundamental structure of space-time. Historically, AI is associated with computer science, but the compleat AI researcher frequently knows a fair amount of psychology, linguistics, neuroscience, mathematics, and possibly some other discipline.}
}
@incollection{PAYDAR2023165,
title = {Chapter 3 - Nuclear power reactions driven radiation hardening environments},
editor = {Ali Zamani Paydar and Seyed Kamal Mousavi Balgehshiri and Bahman Zohuri},
booktitle = {Advanced Reactor Concepts (ARC)},
publisher = {Elsevier},
pages = {165-233},
year = {2023},
isbn = {978-0-443-18989-0},
doi = {https://doi.org/10.1016/B978-0-443-18989-0.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318989000003X},
author = {Ali Zamani Paydar and Seyed Kamal Mousavi Balgehshiri and Bahman Zohuri},
keywords = {Nuclear power plants, Design basis accident, Loss of coolant accident, Shielding, Neutron reflector, Nuclear submarines, Halving thickness values, Artificial intelligence},
abstract = {Radiation hardening, also known as “rad hardening,” and radiation survivability testing are of critical importance to defense, aerospace, and energy industries. Everyone knows that excessive exposure to radiation can cause severe damage to living things, but high radiation levels can also cause radiation damage to other objects, especially electronics. Ionizing radiation in particular, including directly ionizing radiation such as alpha and beta particles and indirectly ionizing radiation such as gamma rays and neutron radiation, is profoundly damaging to the semiconductors that make up the backbone of all modern electronics. Just one charged particle can interfere with thousands of electrons, causing signal noise, disrupting digital circuits, and even causing permanent physical radiation damage. Radiation hardening involves designing radiation-tolerant electronics and components that are tolerant of the massive levels of ionizing radiation, such as cosmic outer space radiation, X-ray radiation in medical or security environments, and high energy radiation within nuclear power plants. In order to test these components and determine whether they are sufficiently hardened, radiation-hardened electronics manufacturers perform rigorous testing as part of their product manufacturing processes. Components which pass these tests go into production and can be described as “radiation-hardened”; components that do not go back to design.}
}
@article{PINSKI2024100062,
title = {AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100062},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100062},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000227},
author = {Marc Pinski and Alexander Benlian},
keywords = {Systematic literature review, Scoping literature review, Artificial intelligence literacy, Learning methods, AI literacy components, AI literacy effects},
abstract = {The rapid advancement of artificial intelligence (AI) has brought transformative changes to various aspects of human life, leading to an exponential increase in the number of AI users. The broad access and usage of AI enable immense benefits but also give rise to significant challenges. One way for AI users to address these challenges is to develop AI literacy, referring to human proficiency in different subject areas of AI that enable purposeful, efficient, and ethical usage of AI technologies. This study aims to comprehensively understand and structure the research on AI literacy for AI users through a systematic, scoping literature review. Therefore, we synthesize the literature, provide a conceptual framework, and develop a research agenda. Our review paper holistically assesses the fragmented AI literacy research landscape (68 papers) while critically examining its specificity to different user groups and its distinction from other technology literacies, exposing that research efforts are partly not well integrated. We organize our findings in an overarching conceptual framework structured along the learning methods leading to, the components constituting, and the effects stemming from AI literacy. Our research agenda – oriented along the developed conceptual framework – sheds light on the most promising research opportunities to prepare AI users for an AI-powered future of work and society.}
}
@article{MEYER202013,
title = {Changing Design Education for the 21st Century},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {6},
number = {1},
pages = {13-49},
year = {2020},
note = {Design Education. Part I},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872620300046},
author = {Michael W. Meyer and Don Norman},
keywords = {Design education, Design-driven transformation, Design thinking, Design doing, Major societal challenges, Complex sociotechnical systems, DesignX},
abstract = {Designers are entrusted with increasingly complex and impactful challenges. However, the current system of design education does not always prepare students for these challenges. When we examine what and how our system teaches young designers, we discover that the most valuable elements of the designer’s perspective and process are seldom taught. Instead, some designers grow beyond their education through their experience working in industry, essentially learning by accident. Many design programs still maintain an insular perspective and an inefficient mechanism of tacit knowledge transfer. Meanwhile, skills for developing creative solutions to complex problems are increasingly essential. Organizations are starting to recognize that designers bring something special to this type of work, a rational belief based upon numerous studies that link commercial success to a design-driven approach. So, what are we to do? Other learned professions such as medicine, law, and business provide excellent advice and guidance embedded within their own histories of professionalization. In this article, we borrow from their experiences to recommend a course of action for design. It will not be easy: it will require a study group to make recommendations for a roster of design and educational practices that schools can use to build a curriculum that matches their goals and abilities. And then it will require a conscious effort to bootstrap the design profession toward both a robust practitioner community and an effective professoriate, capable together of fully realizing the value of design in the 21st century. In this article, we lay out that path.}
}
@article{KUGEL1986137,
title = {Thinking may be more than computing},
journal = {Cognition},
volume = {22},
number = {2},
pages = {137-198},
year = {1986},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(86)90057-0},
url = {https://www.sciencedirect.com/science/article/pii/0010027786900570},
author = {Peter Kugel},
abstract = {The uncomputable parts of thinking (if there are any) can be studied in much the same spirit that Turing (1950) suggested for the study of its computable parts. We can develop precise accounts of cognitive processes that, although they involve more than computing, can still be modelled on the machines we call ‘computers’. In this paper, I want to suggest some ways that this might be done, using ideas from the mathematical theory of uncomputability (or Recursion Theory). And I want to suggest some uses to which the resulting models might be put. (The reader more interested in the models and their uses than the mathematics and its theorems, might want to skim or skip the mathematical parts.)
Résumé
Les éléments du raisonnement ne relevant pas du calculable (uncomputable), (s'il en existe), peuvent s'etudier dans I'optique suggérée par Turing (1950) pour l'étude des éléments calculables (computable). On peut rendre compte avec précision des processus cognitifs qui, bien qu'impliquant plus que des calculs, peuvent cependant être modélisés sur ordinateurs. Dans cet article l'auteur propose des modalités pour arriver à ces résultats en utilisant les idées de la théorie mathdmatique de la Récursion (uncomputability). L'auteur suggère aussi des utilisations pour les modéles que en découlent (Il est possible au lecteur plus intéressé par les modèles et leurs utilisations que par les mathématiques et les théorèmes de passer rapidement sur la partie mathématique ou d'omettre de la lire.)}
}
@incollection{2017535,
title = {Index},
editor = {David B. Kirk and Wen-mei W. Hwu},
booktitle = {Programming Massively Parallel Processors (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {535-550},
year = {2017},
isbn = {978-0-12-811986-0},
doi = {https://doi.org/10.1016/B978-0-12-811986-0.00038-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119860000388}
}
@incollection{FREWIN2012209,
title = {Chapter 6 - Biocompatibility of SiC for Neurological Applications},
editor = {Stephen E. Saddow},
booktitle = {Silicon Carbide Biotechnology},
publisher = {Elsevier},
address = {Oxford},
pages = {209-256},
year = {2012},
isbn = {978-0-12-385906-8},
doi = {https://doi.org/10.1016/B978-0-12-385906-8.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859068000064},
author = {Christopher L. Frewin and Chris Locke and Stephen E. Saddow and Edwin J. Weeber},
keywords = {Central nervous system, glial scarring, biocompatibility, in vitro, PC12, H4, neuron, MTT assay, live cell atomic force microscopy, silicon, 3C-SiC, nanocrystalline diamond, in vivo, long-term implantation},
abstract = {Publisher Summary
This chapter introduces the research that done at the University of South Florida to determine the biocompatibility of single-crystal SiC and related materials with neuronal and glial cells to determine whether they are possible candidate materials for the construction of a neuronal implantation device. It describes the basics about the CNS, how it is organized, and its major cellular constituents are described and their purposes within the CNS. Following this, it considers the research investigating the in vitro biocompatibility of 3C-SiC and a related material, nanocrystalline diamond (NCD), with immortalized neuronal and CNS cell lines. The in vitro reaction of primary derived neurons with 3C-SiC is also detailed, followed by initial in vivo testing of 3C-SiC within the brain of a wild-type mouse. The central nervous system (CNS) is composed of the brain and spinal cord. Blood is not in direct contact with the extracellular fluid within the CNS because of restrictive endothelial cells that create a tight junction known as the blood–brain barrier. Any physical biomedical device that would interact with the CNS has to deal with cells that perform functions similar to their leukocyte counterparts found within the bloodstream. The therapeutic utility provided by these devices is that they could be utilized as a platform for drug delivery behind the blood–brain barrier to specific CNS areas, or to transport neuronal factors or even other cells to the CNS to assist in the repair of the neural system damaged by trauma and disease.}
}